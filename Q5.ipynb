{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport wandb\nfrom Levenshtein import distance\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom matplotlib.font_manager import FontProperties\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nimport time\nimport math\nimport argparse\nfrom types import SimpleNamespace\nfrom copy import deepcopy\nimport itertools\n# functions with comments explained in the script file\n# no redundant comments here\n# only new functions are explained here in comments","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:36.349490Z","iopub.execute_input":"2023-05-23T14:00:36.349860Z","iopub.status.idle":"2023-05-23T14:00:36.358203Z","shell.execute_reply.started":"2023-05-23T14:00:36.349832Z","shell.execute_reply":"2023-05-23T14:00:36.357303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device = \", device)\n\n# Paste your own key\nwandb.login()\n\nSOS_token = \"@\"\nEOS_token = \"#\"\nPAD_token = \"^\"\nUNK_token = \"$\"\n\nSOS_idx = 0\nEOS_idx = 1\nPAD_idx = 2\nUNK_idx = 3\n\nbatch_size = 32\nlanguages = [\"asm\", \"ben\", \"brx\", \"guj\", \"hin\", \"kan\", \"kas\", \"kok\", \"mai\", \"mal\", \"mar\", \"mni\", \"ori\", \"pan\", \"san\", \"sid\", \"tam\", \"tel\", \"urd\"]\nbest_model_path = '/kaggle/working/best_model_attn.pth'\ntest_pred_path = '/kaggle/working/pred_attn.csv'","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:41.622471Z","iopub.execute_input":"2023-05-23T14:00:41.622827Z","iopub.status.idle":"2023-05-23T14:00:44.774972Z","shell.execute_reply.started":"2023-05-23T14:00:41.622798Z","shell.execute_reply":"2023-05-23T14:00:44.774046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timeInMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    s = format(s, \".0f\")\n    return str(m) + \"m \" + str(s) + \"s\"","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:49.476142Z","iopub.execute_input":"2023-05-23T14:00:49.476631Z","iopub.status.idle":"2023-05-23T14:00:49.487752Z","shell.execute_reply.started":"2023-05-23T14:00:49.476591Z","shell.execute_reply":"2023-05-23T14:00:49.486408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Script:\n    def __init__(self, name):\n        self.name = name\n        self.char2index = {SOS_token: SOS_idx, EOS_token: EOS_idx, PAD_token: PAD_idx, UNK_token: UNK_idx}\n        self.char2count = {}\n        self.index2char = {SOS_idx: SOS_token, EOS_idx: EOS_token, PAD_idx: PAD_token, UNK_idx: UNK_token}\n        self.n_chars = 4  # Count SOS, EOS, PAD and UNK\n\n    def addWord(self, word):\n        for char in word:\n            self.addChar(char)\n\n    def addChar(self, char):\n        if char not in self.char2index:\n            self.char2index[char] = self.n_chars\n            self.char2count[char] = 1\n            self.index2char[self.n_chars] = char\n            self.n_chars += 1\n        else:\n            self.char2count[char] += 1","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:51.649800Z","iopub.execute_input":"2023-05-23T14:00:51.650158Z","iopub.status.idle":"2023-05-23T14:00:51.657472Z","shell.execute_reply.started":"2023-05-23T14:00:51.650129Z","shell.execute_reply":"2023-05-23T14:00:51.656432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepareVocab(data, in_scr=\"lat\", out_scr=\"dev\"):\n    input_vocab = Script(in_scr)\n    output_vocab = Script(out_scr)\n    \n    for pair in data:\n        input_vocab.addWord(pair[0])\n        output_vocab.addWord(pair[1])\n    \n    return input_vocab, output_vocab\n\ndef tensorFromWord(word, vocab, sos=False, eos=False):\n    char_list = []\n    if sos:\n        char_list.append(vocab.char2index[SOS_token])\n    for char in word:\n        if char in vocab.char2index:\n            char_list.append(vocab.char2index[char])\n        else:\n            char_list.append(vocab.char2index[UNK_token])\n    if eos:\n        char_list.append(vocab.char2index[EOS_token])\n    char_tensor = torch.tensor(char_list, dtype=torch.long)\n    return char_tensor\n\ndef processData(data, vocab, sos=False, eos=False):\n    tensor_list = []\n    for word in data:\n        word_tensor = tensorFromWord(word, vocab, sos, eos)\n        tensor_list.append(word_tensor)\n    word_tensor_pad = pad_sequence(tensor_list, padding_value=PAD_idx, batch_first=True)\n    return word_tensor_pad\n\ndef wordFromTensor(word_tensor, vocab):\n    word = \"\"\n    for idx in word_tensor:\n        if idx == EOS_idx:\n            break\n        if idx >= UNK_idx:\n            word += vocab.index2char[idx.item()]\n    return word","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:53.483573Z","iopub.execute_input":"2023-05-23T14:00:53.483936Z","iopub.status.idle":"2023-05-23T14:00:53.498672Z","shell.execute_reply.started":"2023-05-23T14:00:53.483908Z","shell.execute_reply":"2023-05-23T14:00:53.496429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, cell_type, input_size, embedding_size, hidden_size, num_layers, dp, bidir=False):\n        super(Encoder, self).__init__()\n        self.cell_type = cell_type\n        self.input_size = input_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(dp)\n        self.bidir = bidir\n        \n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n        if self.num_layers == 1:\n            dp = 0.0\n        if self.cell_type == \"RNN\":\n            self.cell = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers, dropout=dp, bidirectional=self.bidir)\n        elif self.cell_type == \"GRU\":\n            self.cell = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers, dropout=dp, bidirectional=self.bidir)\n        elif self.cell_type == \"LSTM\":\n            self.cell = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=dp, bidirectional=self.bidir)\n\n    def forward(self, x):\n\n        embedding = self.dropout(self.embedding(x))\n        \n        cell = None\n        if self.cell_type == \"LSTM\":\n            outputs, (hidden, cell) = self.cell(embedding)\n            if self.bidir:\n                b_sz = cell.size(1)\n                cell = cell.view(self.num_layers, 2, b_sz, -1)\n                cell = cell[-1]\n                cell = cell.mean(axis=0)\n            else:\n                cell = cell[-1,:,:]\n            cell = cell.unsqueeze(0)\n        else:\n            outputs, hidden = self.cell(embedding)\n        \n        if self.bidir:\n            b_sz = hidden.size(1)\n            hidden = hidden.view(self.num_layers, 2, b_sz, -1)\n            hidden = hidden[-1]\n            hidden = hidden.mean(axis=0)\n            outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        else:\n            hidden = hidden[-1,:,:]\n        hidden = hidden.unsqueeze(0)\n\n        return outputs, hidden, cell","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:55.384144Z","iopub.execute_input":"2023-05-23T14:00:55.384523Z","iopub.status.idle":"2023-05-23T14:00:55.399079Z","shell.execute_reply.started":"2023-05-23T14:00:55.384490Z","shell.execute_reply":"2023-05-23T14:00:55.398070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttnDecoder(nn.Module):\n    def __init__(\n        self, cell_type, input_size, embedding_size, hidden_size, output_size, num_layers, dp\n    ):\n        super(AttnDecoder, self).__init__()\n        self.cell_type = cell_type\n        self.input_size = input_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.attn_fc = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.dropout = nn.Dropout(dp)\n\n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n        if self.num_layers == 1:\n            dp = 0.0\n        if self.cell_type == \"RNN\":\n            self.cell = nn.RNN(self.embedding_size + self.hidden_size, self.hidden_size, self.num_layers, dropout=dp)\n        elif self.cell_type == \"GRU\":\n            self.cell = nn.GRU(self.embedding_size + self.hidden_size, self.hidden_size, self.num_layers, dropout=dp)\n        elif self.cell_type == \"LSTM\":\n            self.cell = nn.LSTM(self.embedding_size + self.hidden_size, self.hidden_size, self.num_layers, dropout=dp)\n        self.fc = nn.Linear(self.hidden_size * 2, self.output_size)\n\n    def forward(self, x, enc_out, hidden, cell):\n        x = x.unsqueeze(0)\n\n        embedding = self.dropout(self.embedding(x))\n        \n        #---------------------------------------\n        enc_out_fc = self.attn_fc(enc_out)\n        last_hidden = hidden[-1:]\n        score = enc_out_fc.permute(1, 0, 2) @ last_hidden.permute(1, 2, 0)\n        score = score.permute(2, 0, 1)\n        attn_w = F.softmax(score, dim=2)\n        context = attn_w.permute(1, 0, 2) @ enc_out.permute(1, 0, 2)\n        context = context.permute(1, 0, 2)\n        #----------------------------------------\n        \n        if self.cell_type == \"LSTM\":\n            outputs, (hidden, cell) = self.cell(torch.cat([embedding, context], dim=2), (hidden, cell))\n        else:\n            outputs, hidden = self.cell(torch.cat([embedding, context], dim=2), hidden)\n\n        predictions = self.fc(torch.cat([outputs, context], dim=2))\n        \n        predictions = predictions.squeeze(0)\n        attn_w = attn_w.squeeze(0)\n\n        return predictions, hidden, cell, attn_w","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:00:57.953159Z","iopub.execute_input":"2023-05-23T14:00:57.953536Z","iopub.status.idle":"2023-05-23T14:00:57.970165Z","shell.execute_reply.started":"2023-05-23T14:00:57.953507Z","shell.execute_reply":"2023-05-23T14:00:57.968981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_sz = source.shape[1]\n        source_len = source.shape[0]\n        target_len = target.shape[0]\n        target_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(target_len, batch_sz, target_vocab_size).to(device)\n\n        enc_out, hidden, cell = self.encoder(source)\n        hidden = hidden.repeat(self.decoder.num_layers,1,1)\n        if self.decoder.cell_type == \"LSTM\":\n            cell = cell.repeat(self.decoder.num_layers,1,1)\n            \n        attn_matrix = torch.zeros(target_len, batch_sz, source_len).to(device)\n        \n        x = target[0]\n\n        for t in range(1, target_len):\n            \n            output, hidden, cell, attn_w = self.decoder(x, enc_out, hidden, cell)\n\n            \n            outputs[t] = output\n            attn_matrix[t] = attn_w\n\n            \n            best_guess = output.argmax(dim=1)\n\n            \n            x = target[t] if random.random() < teacher_force_ratio else best_guess\n\n        return outputs, attn_matrix","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:01.197429Z","iopub.execute_input":"2023-05-23T14:01:01.197940Z","iopub.status.idle":"2023-05-23T14:01:01.212300Z","shell.execute_reply.started":"2023-05-23T14:01:01.197879Z","shell.execute_reply":"2023-05-23T14:01:01.210640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sum_accuracy(preds, target):\n    num_equal_columns = torch.logical_or(preds == target, target == PAD_idx).all(dim=0).sum().item()\n    return num_equal_columns","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:03.680293Z","iopub.execute_input":"2023-05-23T14:01:03.680687Z","iopub.status.idle":"2023-05-23T14:01:03.686135Z","shell.execute_reply.started":"2023-05-23T14:01:03.680656Z","shell.execute_reply":"2023-05-23T14:01:03.685114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot attention matrix\ndef plotAttn(model, input_seq, target_seq, src_vocab, tgt_vocab):\n    # find the output and attention matrix for 9 inputs in the input_seq and target_seq tensor\n    model.eval()\n    \n    b_sz = input_seq.size(0)\n    max_src_len = input_seq.size(1)\n    max_tgt_len = target_seq.size(1)\n    \n    with torch.no_grad():\n        \n        input_seq = input_seq.T.to(device)\n        target_seq = target_seq.T.to(device)\n\n        \n        output, attn_matrix = model(input_seq, target_seq, teacher_force_ratio=0.0)\n        \n        # find the predicted sequence from the output\n        # drop 1st row (for SOS)\n        output = output[1:]\n        attn_matrix = attn_matrix[1:]\n        pred_seq = output.argmax(dim=2)\n        \n        # convert attention matrix to (b_sz, tgt_len, src_len)\n        attn_matrix = attn_matrix.permute(1, 0, 2)\n        input_seq = input_seq.T\n        pred_seq = pred_seq.T\n        \n        # plot the attention matrix\n        fig, axes = plt.subplots(3, 3, figsize=(15,15))\n        fig.text(0.5, 0.04, 'Decoder Output', ha='center', size=14)\n        fig.text(0.04, 0.5, 'Encoder Input', va='center', rotation='vertical', size=14)\n        fig.suptitle(\"Attention Heatmaps\", fontsize=16)\n        fig.tight_layout(pad=5.0)\n        fig.subplots_adjust(top=0.90)\n        \n        # font to print Devanagari script\n        font_prop = FontProperties(fname='/kaggle/input/mangal-font/Mangal.TTF')\n        \n        axes = axes.ravel()\n        for i in range(b_sz):\n            src_len = max_src_len\n            pred_len = max_tgt_len - 1\n            # find the index where the sequence stops <EOS>\n            for j in range(max_src_len):\n                if input_seq[i][j].item() == EOS_idx:\n                    src_len = j+1\n                    break\n            \n            for j in range(max_tgt_len - 1):\n                if pred_seq[i][j].item() == EOS_idx:\n                    pred_len = j+1\n                    break\n            # remove the indices coresponding to padding characters from the attention matrix\n            local_attn = attn_matrix[i]\n            local_attn = local_attn[:pred_len]\n            local_attn = local_attn[:, :src_len]\n            # convert attention matrix to (b_sz, tgt_len, src_len)\n            local_attn = local_attn.T\n            local_attn = local_attn.cpu()\n            \n            xticklabels = []\n            for j in range(pred_len):\n                xticklabels.append(tgt_vocab.index2char[pred_seq[i][j].item()])\n#             xticklabels.append('<e>')\n            yticklabels = []\n            for j in range(src_len):\n                yticklabels.append(src_vocab.index2char[input_seq[i][j].item()])\n#             yticklabels.append('<e>')\n            sns.heatmap(local_attn, ax=axes[i], cmap='magma', cbar=False, vmin=0.0, vmax=1.0)\n            axes[i].xaxis.set_major_formatter(ticker.NullFormatter())\n            axes[i].xaxis.set_minor_locator(ticker.FixedLocator([n + 0.5 for n in range(pred_len)]))\n            axes[i].xaxis.set_minor_formatter(ticker.FixedFormatter(xticklabels))\n            axes[i].yaxis.set_major_formatter(ticker.NullFormatter())\n            axes[i].yaxis.set_minor_locator(ticker.FixedLocator([n + 0.5 for n in range(src_len)]))\n            axes[i].yaxis.set_minor_formatter(ticker.FixedFormatter(yticklabels))\n#             axes[i].set(xticks=[n + 0.5 for n in range(pred_len)], yticks=[n + 0.5 for n in range(src_len)])\n            axes[i].set_yticklabels(yticklabels, rotation=0, fontdict={'fontsize':12})  \n            axes[i].set_xticklabels(xticklabels, fontproperties = font_prop, fontdict={'fontsize':12})\n            axes[i].xaxis.tick_top()\n#         plt.tight_layout()\n        plt.savefig('/kaggle/working/attn_hmap.png')\n        wandb.log({'Attention Heatmap' : wandb.Image(fig)})\n        \n        return","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:32:38.343791Z","iopub.execute_input":"2023-05-23T14:32:38.344199Z","iopub.status.idle":"2023-05-23T14:32:38.371295Z","shell.execute_reply.started":"2023-05-23T14:32:38.344168Z","shell.execute_reply":"2023-05-23T14:32:38.370231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluateModel(model, dataloader, criterion, b_sz=32):\n    model.eval()\n    \n    n_data = len(dataloader) * b_sz\n    loss_epoch = 0\n    n_correct = 0\n    \n    with torch.no_grad():\n        for (input_seq, target_seq) in dataloader:\n            \n            input_seq = input_seq.T.to(device)\n            target_seq = target_seq.T.to(device)\n\n            \n            output, attn_matrix = model(input_seq, target_seq, teacher_force_ratio=0.0)\n            \n            pred_seq = output.argmax(dim=2)\n            n_correct += sum_accuracy(pred_seq, target_seq)\n\n            \n            output = output[1:].reshape(-1, output.shape[2])\n            target = target_seq[1:].reshape(-1)\n            \n            loss = criterion(output, target)\n\n            loss_epoch += loss.item()\n        \n        acc = n_correct / n_data\n        acc = acc * 100.0\n        loss_epoch /= len(dataloader)\n        return loss_epoch, acc","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:10.172831Z","iopub.execute_input":"2023-05-23T14:01:10.173199Z","iopub.status.idle":"2023-05-23T14:01:10.182954Z","shell.execute_reply.started":"2023-05-23T14:01:10.173151Z","shell.execute_reply":"2023-05-23T14:01:10.181502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveAndEvaluate(model, dataloader, criterion, df, vocab, b_sz=32):\n    results = []\n    model.eval()\n    \n    n_data = len(dataloader) * b_sz\n    loss_epoch = 0\n    n_correct = 0\n    \n    with torch.no_grad():\n        for (input_seq, target_seq) in dataloader:\n            \n            input_seq = input_seq.T.to(device)\n            target_seq = target_seq.T.to(device)\n\n            \n            output, attn_matrix = model(input_seq, target_seq, teacher_force_ratio=0.0)\n            \n            pred_seq = output.argmax(dim=2)\n            n_correct += sum_accuracy(pred_seq, target_seq)\n\n            \n            output = output[1:].reshape(-1, output.shape[2])\n            target = target_seq[1:].reshape(-1)\n            \n            loss = criterion(output, target)\n\n            loss_epoch += loss.item()\n            \n            pred_seq = pred_seq.T\n            for idx in range(b_sz):\n                word = wordFromTensor(pred_seq[idx], vocab)\n                results.append(word)\n                \n        acc = n_correct / n_data\n        acc = acc * 100.0\n        loss_epoch /= len(dataloader)\n        \n        df[2] = results\n        new_column_names = {0: 'Source', 1: 'Target', 2: 'Predicted'}\n        df = df.rename(columns=new_column_names)\n        df.to_csv(test_pred_path, index=False)\n        \n        return loss_epoch, acc, df","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:12.236143Z","iopub.execute_input":"2023-05-23T14:01:12.236827Z","iopub.status.idle":"2023-05-23T14:01:12.248470Z","shell.execute_reply.started":"2023-05-23T14:01:12.236784Z","shell.execute_reply":"2023-05-23T14:01:12.247390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainModel(model, criterion, optimizer, train_dataloader, valid_dataloader, num_epochs, batch_size=32):\n    start = time.time()\n    min_val_loss = 10000.0\n    min_val_epoch = 0\n    trigger = 0\n    \n    best_model_state = deepcopy(model.state_dict())\n    \n    tr_loss_list = []\n    tr_acc_list = []\n    val_loss_list = []\n    val_acc_list = []\n    for epoch in range(num_epochs):\n        print(f\"[Epoch {epoch+1} / {num_epochs}]\")\n        model.train()\n\n        for (input_seq, target_seq) in train_dataloader:\n            \n            input_seq = input_seq.T.to(device)\n            target_seq = target_seq.T.to(device)\n\n            \n            output, attn_matrix = model(input_seq, target_seq)\n            \n            output = output[1:].reshape(-1, output.shape[2])\n            target = target_seq[1:].reshape(-1)\n\n            optimizer.zero_grad()\n            loss = criterion(output, target)\n\n            \n            loss.backward()\n\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n            \n            optimizer.step()\n\n        #-----------------------------------------------\n        # Train loss and accuracy\n        tr_loss, tr_acc = evaluateModel(model, train_dataloader, criterion, batch_size)\n        tr_loss_list.append(tr_loss)\n        tr_acc_list.append(tr_acc)\n        \n        print(f\"Training Loss: {tr_loss:.2f}\")\n        print(f\"Training Accuracy: {tr_acc:.2f}\")\n\n        #-----------------------------------------------\n        # Valid loss and accuracy\n        val_loss, val_acc = evaluateModel(model, valid_dataloader, criterion, batch_size)\n        val_loss_list.append(val_loss)\n        val_acc_list.append(val_acc)\n        \n        print(f\"Validation Loss: {val_loss:.2f}\")\n        print(f\"Validation Accuracy: {val_acc:.2f}\")\n\n        if val_loss <= min_val_loss:\n            best_model_state = deepcopy(model.state_dict())\n            trigger = 0\n            min_val_loss = val_loss\n            min_val_epoch = epoch\n        else:\n            trigger += 1\n        \n        end = time.time()\n        print(\"Time: \", timeInMinutes(end-start))\n        print(\"----------------------------------\")\n        \n        if trigger == 5:\n            print('Early stopping!')\n            break\n    \n    print('Saving the best model...')\n    torch.save(best_model_state, best_model_path)\n    print('Best model saved.')\n    return","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:12.882748Z","iopub.execute_input":"2023-05-23T14:01:12.883753Z","iopub.status.idle":"2023-05-23T14:01:12.900604Z","shell.execute_reply.started":"2023-05-23T14:01:12.883709Z","shell.execute_reply":"2023-05-23T14:01:12.899559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_path = '/kaggle/input/eng-hin/hin_train.csv'\nvalid_data_path = '/kaggle/input/eng-hin/hin_valid.csv'\ntest_data_path = '/kaggle/input/eng-hin/hin_test.csv'\n\ntrain_data = pd.read_csv(train_data_path, sep=',', header=None).values\ntest_data = pd.read_csv(test_data_path, sep=',', header=None).values\nvalid_data = pd.read_csv(valid_data_path, sep=',', header=None).values","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:13.726154Z","iopub.execute_input":"2023-05-23T14:01:13.726543Z","iopub.status.idle":"2023-05-23T14:01:13.817264Z","shell.execute_reply.started":"2023-05-23T14:01:13.726513Z","shell.execute_reply":"2023-05-23T14:01:13.816250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build vocabulary\nx_vocab, y_vocab = prepareVocab(train_data)\n\nprint('Number of characters in Source Vocab :', x_vocab.n_chars-4)\nprint('Number of characters in Target Vocab :', y_vocab.n_chars-4)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:14.468887Z","iopub.execute_input":"2023-05-23T14:01:14.469280Z","iopub.status.idle":"2023-05-23T14:01:14.888815Z","shell.execute_reply.started":"2023-05-23T14:01:14.469252Z","shell.execute_reply":"2023-05-23T14:01:14.887576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = processData(train_data[:,0], x_vocab, eos=True).to(device=device)\nx_test = processData(test_data[:,0], x_vocab, eos=True).to(device=device)\nx_valid = processData(valid_data[:,0], x_vocab, eos=True).to(device=device)\n\ny_train = processData(train_data[:,1], y_vocab, sos=True, eos=True).to(device=device)\ny_test = processData(test_data[:,1], y_vocab, sos=True, eos=True).to(device=device)\ny_valid = processData(valid_data[:,1], y_vocab, sos=True, eos=True).to(device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:17.324173Z","iopub.execute_input":"2023-05-23T14:01:17.324555Z","iopub.status.idle":"2023-05-23T14:01:19.618241Z","shell.execute_reply.started":"2023-05-23T14:01:17.324523Z","shell.execute_reply":"2023-05-23T14:01:19.617252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train = x_train.size(0)\nn_valid = x_valid.size(0)\nn_test = x_test.size(0)\nprint('Number of Training Sequences :', n_train)\nprint('Number of Validation Sequences :', n_valid)\nprint('Number of Test Sequences :', n_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:19.622804Z","iopub.execute_input":"2023-05-23T14:01:19.623088Z","iopub.status.idle":"2023-05-23T14:01:19.631718Z","shell.execute_reply.started":"2023-05-23T14:01:19.623064Z","shell.execute_reply":"2023-05-23T14:01:19.630785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_src_len = x_test.size(1)\nmax_tgt_len = y_test.size(1)\nprint(max_src_len, max_tgt_len)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:21.784474Z","iopub.execute_input":"2023-05-23T14:01:21.784832Z","iopub.status.idle":"2023-05-23T14:01:21.791044Z","shell.execute_reply.started":"2023-05-23T14:01:21.784804Z","shell.execute_reply":"2023-05-23T14:01:21.788709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TensorDataset(x_train, y_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nvalid_dataset = TensorDataset(x_valid, y_valid)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n\ntest_dataset = TensorDataset(x_test, y_test)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:24.504050Z","iopub.execute_input":"2023-05-23T14:01:24.504428Z","iopub.status.idle":"2023-05-23T14:01:24.511744Z","shell.execute_reply.started":"2023-05-23T14:01:24.504396Z","shell.execute_reply":"2023-05-23T14:01:24.510833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 15\nlearning_rate = 0.001\n\n# Fixed parameters for encoder and decoder\ninput_size_encoder = x_vocab.n_chars\ninput_size_decoder = y_vocab.n_chars\noutput_size = input_size_decoder","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:28.487810Z","iopub.execute_input":"2023-05-23T14:01:28.488177Z","iopub.status.idle":"2023-05-23T14:01:28.494194Z","shell.execute_reply.started":"2023-05-23T14:01:28.488145Z","shell.execute_reply":"2023-05-23T14:01:28.493254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best hyperparameter configuration\nparams = {\n    'wandb_project' : 'dl_ass_3_q_5',\n    'wandb_entity' : 'cs22m059',\n    'dataset' : 'hin',\n    'cell_type' : 'LSTM',\n    'embedding_size' : 128,\n    'hidden_size' : 256,\n    'enc_num_layers' : 3,\n    'dec_num_layers' : 3,\n    'dropout' : 0.2,\n    'bidirectional' : 'Yes'\n}\n#--------------------------------------------\n# initialize wandb with given params\nwandb.init(project = params['wandb_project'], config = params)\nprint(\"Provided hyperparameters = \", params)\nparams = SimpleNamespace(**params)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:01:32.447666Z","iopub.execute_input":"2023-05-23T14:01:32.448023Z","iopub.status.idle":"2023-05-23T14:02:04.522139Z","shell.execute_reply.started":"2023-05-23T14:01:32.447992Z","shell.execute_reply":"2023-05-23T14:02:04.521138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Building the model...\")\n# Model hyperparameters\ncell_type = params.cell_type\nembedding_size = params.embedding_size\nhidden_size = params.hidden_size  # Needs to be the same for both RNN's\nenc_num_layers = params.enc_num_layers\ndec_num_layers = params.dec_num_layers\ndropout = params.dropout\nbidirectional = True if params.bidirectional == \"Yes\" else False\n\nencoder_net = Encoder(\ncell_type, input_size_encoder, embedding_size, hidden_size, enc_num_layers, dropout, bidirectional).to(device)\n\ndecoder_net = AttnDecoder(\n    cell_type,\n    input_size_decoder,\n    embedding_size,\n    hidden_size,\n    output_size,\n    dec_num_layers,\n    dropout,\n).to(device)\n\nmodel = Seq2Seq(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\ntrainModel(model, criterion, optimizer, train_dataloader, valid_dataloader, num_epochs, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:02:07.179884Z","iopub.execute_input":"2023-05-23T14:02:07.180353Z","iopub.status.idle":"2023-05-23T14:23:06.157567Z","shell.execute_reply.started":"2023-05-23T14:02:07.180317Z","shell.execute_reply":"2023-05-23T14:23:06.156495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(best_model_path))\n\ntest_data_df = pd.read_csv(test_data_path, sep=',', header=None)\ntest_loss, test_acc, pred_df = saveAndEvaluate(model, test_dataloader, criterion, test_data_df, y_vocab, batch_size)\nprint(f\"Test Loss: {test_loss:.2f}\")\nprint(f\"Test Accuracy: {test_acc:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:23:20.519385Z","iopub.execute_input":"2023-05-23T14:23:20.520089Z","iopub.status.idle":"2023-05-23T14:23:25.260456Z","shell.execute_reply.started":"2023-05-23T14:23:20.520053Z","shell.execute_reply":"2023-05-23T14:23:25.259105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find Levenshtein distance for sample inputs and predictions\ndf_sample = pred_df.sample(n=10)\nlev_list = []\nfor ind in df_sample.index:\n    lev_list.append(distance(df_sample['Target'][ind], df_sample['Predicted'][ind]))\ndf_sample['Levenshtein'] = lev_list","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:23:29.397346Z","iopub.execute_input":"2023-05-23T14:23:29.398096Z","iopub.status.idle":"2023-05-23T14:23:29.405012Z","shell.execute_reply.started":"2023-05-23T14:23:29.398062Z","shell.execute_reply":"2023-05-23T14:23:29.403649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display sample inputs and predictions\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(df_sample.columns),\n                fill_color='paleturquoise',\n                align='center',\n                font_size=18,\n                height=30\n                ),\n    cells=dict(values=[df_sample.Source, df_sample.Target, df_sample.Predicted, df_sample.Levenshtein],\n               fill_color='lavender',\n               align='center',\n               font_size=16,\n               height=30))\n])\n\nfig.update_layout(autosize=False, width=700, height=330, margin=dict(l=0,r=0,b=0,t=0))\nfig.show()\n\nwandb.log({'Sample Predictions': fig})\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:23:43.239057Z","iopub.execute_input":"2023-05-23T14:23:43.239451Z","iopub.status.idle":"2023-05-23T14:23:43.279165Z","shell.execute_reply.started":"2023-05-23T14:23:43.239417Z","shell.execute_reply":"2023-05-23T14:23:43.278124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take sample inputs and plot their attention matrix\nidx_list = [1469, 3490, 1480, 904, 623, 36, 234, 201, 58]\nsample_source = torch.zeros(len(idx_list), max_src_len, dtype=torch.long)\nsample_target = torch.zeros(len(idx_list), max_tgt_len, dtype=torch.long)\n\ni = 0\nfor idx in idx_list:\n    # find the batch index and index inside the batch for a given index in the csv file\n    b_idx = int(np.floor(idx/batch_size))\n    d_idx = idx - (b_idx * batch_size)\n    # retreive the data from test dataloader\n    sample = next(itertools.islice(test_dataloader, b_idx, None))\n    sample_source[i] = sample[0][d_idx]\n    sample_target[i] = sample[1][d_idx]\n    i += 1\n\nplotAttn(model, sample_source, sample_target, x_vocab, y_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:32:44.927280Z","iopub.execute_input":"2023-05-23T14:32:44.927689Z","iopub.status.idle":"2023-05-23T14:32:52.448980Z","shell.execute_reply.started":"2023-05-23T14:32:44.927658Z","shell.execute_reply":"2023-05-23T14:32:52.448010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of some wrongly predicted words by vanilla model corrected by attention model\nsource_list = [\n    'foohadta',\n    'ukhrul',\n    'nabz',\n    'tarkvaad',\n    'phabti',\n    'barker',\n    'sanyukt',\n    'ikattha',\n    'achchi',\n    'vipannata',\n    'mckenzie',\n    'samruddhiyon',\n    'abdunnaasir',\n    'dayaalapuraa',\n    'karneshvardhaam',\n    'swaadpremiyon',\n    'naameegiraamee',\n    'manokaamnaaon',\n    'pratirodhaatmak',\n    'hastaantaraneey',\n    'utsaahawarddhan',\n    'shaasanadhikariyon',\n    'properfacebuk',\n    'varnavyavasthaavaadiyon'\n]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:01.485652Z","iopub.execute_input":"2023-05-23T14:34:01.486026Z","iopub.status.idle":"2023-05-23T14:34:01.500341Z","shell.execute_reply.started":"2023-05-23T14:34:01.485996Z","shell.execute_reply":"2023-05-23T14:34:01.498647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load prediction of vanilla model\npred_vnl = pd.read_csv('/kaggle/input/pred-vanilla/pred_vanilla.csv', sep=',', header=None)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:02.978933Z","iopub.execute_input":"2023-05-23T14:34:02.979300Z","iopub.status.idle":"2023-05-23T14:34:03.005300Z","shell.execute_reply.started":"2023-05-23T14:34:02.979269Z","shell.execute_reply":"2023-05-23T14:34:03.003546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the source, target and predicted from the pred_vanilla.csv and attention predictions\n# where source is in the source list\nsrc = []\ntgt = []\nvnl = []\natn = []\ndf_sample = pd.DataFrame()\n\nfor i in range(n_test) :\n    if source_list.count(pred_df['Source'][i]) > 0:\n        src.append(pred_df['Source'][i])\n        tgt.append(pred_df['Target'][i])\n        atn.append(pred_df['Predicted'][i])\n        vnl.append(pred_vnl[2][i+1])","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:05.926301Z","iopub.execute_input":"2023-05-23T14:34:05.926889Z","iopub.status.idle":"2023-05-23T14:34:05.985961Z","shell.execute_reply.started":"2023-05-23T14:34:05.926856Z","shell.execute_reply":"2023-05-23T14:34:05.984693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample['Source'] = src\ndf_sample['Target'] = tgt\ndf_sample['Vanilla'] = vnl\ndf_sample['Attention'] = atn","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:07.309930Z","iopub.execute_input":"2023-05-23T14:34:07.310301Z","iopub.status.idle":"2023-05-23T14:34:07.319693Z","shell.execute_reply.started":"2023-05-23T14:34:07.310270Z","shell.execute_reply":"2023-05-23T14:34:07.318750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display inputs and predictions for the words in source list\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(df_sample.columns),\n                fill_color='paleturquoise',\n                align='center',\n                font_size=18,\n                height=30\n                ),\n    cells=dict(values=[df_sample.Source, df_sample.Target, df_sample.Vanilla, df_sample.Attention],\n               fill_color='lavender',\n               align='center',\n               font_size=16,\n               height=30))\n])\n\nfig.update_layout(autosize=False, width=900, height=750, margin=dict(l=0,r=0,b=0,t=0))\nfig.show()\n\nwandb.log({'Wrong predictions of Vanilla model correctly predicted by Attention model': fig})","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:17.027541Z","iopub.execute_input":"2023-05-23T14:34:17.027899Z","iopub.status.idle":"2023-05-23T14:34:17.065918Z","shell.execute_reply.started":"2023-05-23T14:34:17.027871Z","shell.execute_reply":"2023-05-23T14:34:17.065065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of some wrongly predicted words by both vanilla and attention models\nsource_list = [\n    'wpd',\n    'idea',\n    'tyoiharon',\n    'eyarkandeeshnar',\n    'tekchandani',\n    'rikailleebreshan'\n]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:21.667480Z","iopub.execute_input":"2023-05-23T14:34:21.667836Z","iopub.status.idle":"2023-05-23T14:34:21.674903Z","shell.execute_reply.started":"2023-05-23T14:34:21.667807Z","shell.execute_reply":"2023-05-23T14:34:21.673830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the source, target and predicted from the pred_vanilla.csv and attention predictions\n# where source is in the source list\nsrc = []\ntgt = []\nvnl = []\natn = []\ndf_sample = pd.DataFrame()\n\nfor i in range(n_test) :\n    if source_list.count(pred_df['Source'][i]) > 0:\n        src.append(pred_df['Source'][i])\n        tgt.append(pred_df['Target'][i])\n        atn.append(pred_df['Predicted'][i])\n        vnl.append(pred_vnl[2][i+1])","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:24.973183Z","iopub.execute_input":"2023-05-23T14:34:24.974111Z","iopub.status.idle":"2023-05-23T14:34:25.029608Z","shell.execute_reply.started":"2023-05-23T14:34:24.974068Z","shell.execute_reply":"2023-05-23T14:34:25.028540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample['Source'] = src\ndf_sample['Target'] = tgt\ndf_sample['Vanilla'] = vnl\ndf_sample['Attention'] = atn","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:27.714852Z","iopub.execute_input":"2023-05-23T14:34:27.715866Z","iopub.status.idle":"2023-05-23T14:34:27.726643Z","shell.execute_reply.started":"2023-05-23T14:34:27.715820Z","shell.execute_reply":"2023-05-23T14:34:27.725259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display inputs and predictions for the words in source list\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(df_sample.columns),\n                fill_color='paleturquoise',\n                align='center',\n                font_size=18,\n                height=30\n                ),\n    cells=dict(values=[df_sample.Source, df_sample.Target, df_sample.Vanilla, df_sample.Attention],\n               fill_color='lavender',\n               align='center',\n               font_size=16,\n               height=30))\n])\n\nfig.update_layout(autosize=False, width=700, height=210, margin=dict(l=0,r=0,b=0,t=0))\nfig.show()\n\nwandb.log({'Wrong predictions by both Attetion and Vanilla models' : fig})\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:31.656977Z","iopub.execute_input":"2023-05-23T14:34:31.657327Z","iopub.status.idle":"2023-05-23T14:34:37.423752Z","shell.execute_reply.started":"2023-05-23T14:34:31.657298Z","shell.execute_reply":"2023-05-23T14:34:37.422883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cases where attention made correct predictions but vanilla made wrong\nn_cases = 0\nn_lev_vnl = 0\nfor i in range(n_test) :\n    if pred_df['Target'][i] == pred_df['Predicted'][i] and pred_vnl[2][i+1] != pred_vnl[1][i+1]:\n        n_cases += 1\n        n_lev_vnl += distance(pred_vnl[2][i+1], pred_vnl[1][i+1])\nprint('Number of cases where vanilla wrong and attention correct = ', n_cases)\nprint('Average Vanilla Levenshtein distance = ', n_lev_vnl/n_cases)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:44.800292Z","iopub.execute_input":"2023-05-23T14:34:44.801039Z","iopub.status.idle":"2023-05-23T14:34:44.931317Z","shell.execute_reply.started":"2023-05-23T14:34:44.801001Z","shell.execute_reply":"2023-05-23T14:34:44.930399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cases where attention made wrong predictions but vanilla made correct\nn_cases = 0\nn_lev_atn = 0\nfor i in range(n_test) :\n    if pred_df['Target'][i] != pred_df['Predicted'][i] and pred_vnl[2][i+1] == pred_vnl[1][i+1]:\n        n_cases += 1\n        n_lev_atn += distance(pred_df['Target'][i], pred_df['Predicted'][i])\nprint('Number of cases where vanilla correct and attention wrong = ', n_cases)\nprint('Average Attention Levenshtein distance = ', n_lev_atn/n_cases)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:47.949010Z","iopub.execute_input":"2023-05-23T14:34:47.949376Z","iopub.status.idle":"2023-05-23T14:34:48.090833Z","shell.execute_reply.started":"2023-05-23T14:34:47.949331Z","shell.execute_reply":"2023-05-23T14:34:48.088651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cases where attention and vanilla both made wrong predictions\nn_cases = 0\nn_lev_atn = 0\nn_lev_vnl = 0\nfor i in range(n_test) :\n    if pred_df['Target'][i] != pred_df['Predicted'][i] and pred_vnl[2][i+1] != pred_vnl[1][i+1]:\n        n_cases += 1\n        n_lev_atn += distance(pred_df['Target'][i], pred_df['Predicted'][i])\n        n_lev_vnl += distance(pred_vnl[2][i+1], pred_vnl[1][i+1])\nprint('Number of cases where vanilla and attention both wrong = ', n_cases)\nprint('Average Attention Levenshtein distance = ', n_lev_atn/n_cases)\nprint('Average Vanilla Levenshtein distance = ', n_lev_vnl/n_cases)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:34:50.870917Z","iopub.execute_input":"2023-05-23T14:34:50.871268Z","iopub.status.idle":"2023-05-23T14:34:51.089428Z","shell.execute_reply.started":"2023-05-23T14:34:50.871239Z","shell.execute_reply":"2023-05-23T14:34:51.088417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}